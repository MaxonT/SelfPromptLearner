import streamlit as st
import pandas as pd
import jieba
import re
import json
import matplotlib.pyplot as plt
import numpy as np
from wordcloud import WordCloud
from collections import Counter
from datetime import datetime

# --- å­—ä½“å¤„ç† (Mac ä¹±ç ç»ˆç»“ç‰ˆ) ---
import platform
def get_chinese_font():
    system = platform.system()
    if system == "Darwin": # Mac
        fonts = ["/System/Library/Fonts/PingFang.ttc", 
                 "/System/Library/Fonts/STHeiti Light.ttc",
                 "/System/Library/Fonts/Supplemental/Arial Unicode.ttf"]
        for f in fonts:
            try: 
                open(f)
                return f
            except: continue
    return None # Win/Linux éœ€å¦å¤–å¤„ç†ï¼Œæš‚æ—¶ fallback

font_path = get_chinese_font()
# è®¾ç½® Matplotlib å­—ä½“ä»¥æ”¯æŒä¸­æ–‡
if platform.system() == "Darwin":
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Heiti TC', 'PingFang SC']
else:
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False
# --------------------------------

# é¡µé¢é…ç½®
st.set_page_config(page_title="SPR - ä½ çš„ Prompt ç”»åƒ", layout="wide", page_icon="ğŸ”®")

st.markdown("""
<style>
    .main .block-container { padding-top: 2rem; }
    h1 { color: #2c3e50; }
    h3 { color: #34495e; font-size: 1.2rem; }
    .stMetric { background-color: #f8f9fa; padding: 10px; border-radius: 8px; border: 1px solid #eee; }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ”® SPR é•œåƒç‰ˆï¼šçœ‹è§ä½ çš„æ€ç»´ä¹ æƒ¯")

# ä¾§è¾¹æ ï¼šä¸Šä¼ 
with st.sidebar:
    st.header("ğŸ“¤ æ•°æ®å¯¼å…¥")
    st.info("è¯·ä½¿ç”¨ Chrome æ’ä»¶å¯¼å‡ºçš„ `my_prompts.json` æ–‡ä»¶")
    up = st.file_uploader("æ‹–å…¥æ–‡ä»¶", type=["json", "txt", "jsonl"])
    
    st.markdown("---")
    st.markdown("### éšç§è¯´æ˜")
    st.caption("æ‰€æœ‰è®¡ç®—å‡åœ¨æœ¬åœ°å®Œæˆï¼Œæ•°æ®ä¸ä¸Šä¼ äº‘ç«¯ã€‚")

# æ•°æ®åŠ è½½é€»è¾‘
lines = []
timestamps = []
sources = []

if up:
    try:
        content = up.read().decode('utf-8', errors='ignore')
        
        # 1. å°è¯•è§£ææ–°ç‰ˆæ’ä»¶ JSON [{ts, text, src}, ...]
        if up.name.endswith('.json'):
            try:
                data = json.loads(content)
                if isinstance(data, list) and len(data) > 0:
                    # æ£€æŸ¥æ˜¯ä¸æ˜¯æ’ä»¶æ ¼å¼
                    if 'text' in data[0]: 
                        for item in data:
                            lines.append(item.get('text', ''))
                            ts = item.get('ts', 0)
                            if ts > 0: timestamps.append(datetime.fromtimestamp(ts / 1000))
                            sources.append(item.get('src', 'unknown'))
                    # å…¼å®¹ ChatGPT å®˜æ–¹å¯¼å‡º
                    elif 'mapping' in data[0]:
                         for conv in data:
                            if 'mapping' in conv:
                                for k, v in conv['mapping'].items():
                                    if v['message'] and v['message']['author']['role'] == 'user':
                                        parts = v['message']['content']['parts']
                                        if parts: 
                                            lines.append(str(parts[0]))
                                            # å®˜æ–¹å¯¼å‡ºåŒ…å« create_time
                                            ct = v['message'].get('create_time')
                                            if ct: timestamps.append(datetime.fromtimestamp(ct))
            except json.JSONDecodeError:
                pass # å¯èƒ½æ˜¯å…¶ä»–æ ¼å¼

        # 2. å…¼å®¹æ—§ç‰ˆ TXT / JSONL
        if not lines: 
             if up.name.endswith('.jsonl'):
                for line in content.splitlines():
                    if line.strip():
                        try:
                            msg = json.loads(line)
                            if 'messages' in msg: lines.append(msg['messages'][0]['content'])
                        except: pass
             else:
                lines = [l.strip() for l in content.split('===SPLIT===') if l.strip()]
                if len(lines) < 2:
                     lines = [l.strip() for l in content.splitlines() if l.strip()]

    except Exception as e:
        st.error(f"è§£æå¤±è´¥: {e}")

if not lines:
    st.info("ğŸ‘ˆ è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ æ•°æ®")
    st.stop()

# --- æ•°æ®é¢„å¤„ç† ---
df = pd.DataFrame({"prompt": lines})
df["len"] = df["prompt"].str.len()
if timestamps and len(timestamps) == len(lines):
    df["time"] = timestamps
    df["hour"] = df["time"].dt.hour
    has_time = True
else:
    has_time = False

# åˆ†è¯
def get_words(text):
    return [w for w in jieba.lcut(text) if len(w) > 1 and re.match(r"[\u4e00-\u9fa5a-zA-Z]", w)]

all_text = " ".join(lines)
words = get_words(all_text)
word_counts = Counter(words)

# --- æ ¸å¿ƒæŒ‡æ ‡ ---
c1, c2, c3, c4 = st.columns(4)
c1.metric("ç´¯è®¡ Prompt", f"{len(df)} æ¡")
c2.metric("å¹³å‡é•¿åº¦", f"{int(df['len'].mean())} å­—")
c3.metric("æ€»è¯æ±‡é‡", f"{len(word_counts)} ä¸ª")
most_common_word = word_counts.most_common(1)[0][0] if word_counts else "æ— "
c4.metric("æœ€çˆ±ç”¨çš„è¯", most_common_word)

st.divider()

# --- ç¬¬ä¸€æ’ï¼šè¯äº‘ & äººæ ¼é›·è¾¾ ---
col_cloud, col_radar = st.columns([1.5, 1])

with col_cloud:
    st.subheader("â˜ï¸ ä½ çš„æ€ç»´è¯äº‘")
    
    wc = WordCloud(font_path=font_path, width=800, height=500, background_color="white", 
                   max_words=100, collocations=False).generate(" ".join(words))
    st.image(wc.to_array(), use_column_width=True)

with col_radar:
    st.subheader("ğŸ•¸ï¸ Prompt äººæ ¼é›·è¾¾")
    
    # ç®€å•çš„å…³é”®è¯åˆ†ç±»å™¨
    categories = {
        "ğŸ’» ç¼–ç¨‹å¼€å‘": ["ä»£ç ", "code", "å‡½æ•°", "æŠ¥é”™", "bug", "python", "js", "react", "sql", "api", "å†™ä¸€ä¸ª", "å®ç°"],
        "ğŸ“ å†…å®¹åˆ›ä½œ": ["æ–‡æ¡ˆ", "æ–‡ç« ", "å‘¨æŠ¥", "æ€»ç»“", "æ‰©å†™", "æ¶¦è‰²", "å¤§çº²", "æ ‡é¢˜", "ç¿»è¯‘", "é‚®ä»¶"],
        "ğŸ§  é€»è¾‘åˆ†æ": ["åˆ†æ", "åŸå› ", "åŒºåˆ«", "æ¯”è¾ƒ", "è¯„ä»·", "ä¼˜ç¼ºç‚¹", "å»ºè®®", "æ–¹æ¡ˆ", "æ€ç»´å¯¼å›¾"],
        "ğŸ“ çŸ¥è¯†å­¦ä¹ ": ["è§£é‡Š", "ä»‹ç»", "æ˜¯ä»€ä¹ˆ", "å«ä¹‰", "åŸç†", "æ•™ç¨‹", "å­¦ä¹ ", "å¦‚ä½•"],
        "ğŸ¨ åˆ›æ„è„‘æš´": ["åˆ›æ„", "ç‚¹å­", "æ•…äº‹", "è®¾æƒ³", "å¦‚æœ", "ç”Ÿæˆ", "è®¾è®¡"]
    }
    
    scores = {k: 0 for k in categories}
    for w in words:
        w_lower = w.lower()
        for cat, keywords in categories.items():
            if w_lower in keywords:
                scores[cat] += 1
    
    # å½’ä¸€åŒ–
    total_score = sum(scores.values()) or 1
    labels = list(scores.keys())
    values = [s/total_score for s in scores.values()]
    # é—­åˆé›·è¾¾å›¾
    values.append(values[0])
    angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False).tolist()
    angles.append(angles[0])
    
    fig_radar = plt.figure(figsize=(4, 4), facecolor='#f8f9fa')
    ax = fig_radar.add_subplot(111, polar=True, facecolor='#f8f9fa')
    ax.plot(angles, values, 'o-', linewidth=2, color='#fb5607')
    ax.fill(angles, values, alpha=0.25, color='#fb5607')
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(labels, fontsize=10)
    # éšè—yè½´åˆ»åº¦
    ax.set_yticklabels([])
    ax.spines['polar'].set_visible(False)
    st.pyplot(fig_radar)

st.divider()

# --- ç¬¬äºŒæ’ï¼šæ—¶é—´åˆ†å¸ƒ & é•¿åº¦åˆ†å¸ƒ ---
if has_time:
    st.subheader("ğŸ“… ä½ çš„æ´»è·ƒæ—¶æ®µ")
    c_time, c_len = st.columns(2)
    
    with c_time:
        hour_counts = df['hour'].value_counts().sort_index()
        # è¡¥å…¨ 24 å°æ—¶
        for h in range(24):
            if h not in hour_counts: hour_counts[h] = 0
        hour_counts = hour_counts.sort_index()
        
        fig_time, ax_time = plt.subplots(figsize=(10, 4))
        ax_time.bar(hour_counts.index, hour_counts.values, color='#3a86ff', alpha=0.7)
        ax_time.set_xticks(range(0, 24, 2))
        ax_time.set_xlabel("å°æ—¶ (0-23)")
        ax_time.set_ylabel("Prompt æ•°é‡")
        ax_time.set_title("24å°æ—¶æ´»è·ƒåº¦çƒ­åŠ›")
        ax_time.spines['top'].set_visible(False)
        ax_time.spines['right'].set_visible(False)
        st.pyplot(fig_time)
        
    with c_len:
         fig_len, ax_len = plt.subplots(figsize=(10, 4))
         ax_len.hist(df["len"], bins=30, color="#ffbe0b", alpha=0.8)
         ax_len.set_title("Prompt é•¿åº¦åˆ†å¸ƒ")
         ax_len.set_xlabel("å­—ç¬¦æ•°")
         ax_len.spines['top'].set_visible(False)
         ax_len.spines['right'].set_visible(False)
         st.pyplot(fig_len)

else:
    st.warning("âš ï¸ å½“å‰æ•°æ®ä¸åŒ…å«æ—¶é—´ä¿¡æ¯ï¼Œæ— æ³•æ˜¾ç¤ºæ´»è·ƒæ—¶æ®µåˆ†æã€‚è¯·ä½¿ç”¨æ–°ç‰ˆæ’ä»¶é‡æ–°å¯¼å‡º JSONã€‚")
    st.subheader("ğŸ“ Prompt é•¿åº¦åˆ†å¸ƒ")
    fig_len, ax_len = plt.subplots(figsize=(10, 4))
    ax_len.hist(df["len"], bins=30, color="#ffbe0b", alpha=0.8)
    st.pyplot(fig_len)

# --- åº•éƒ¨ï¼šé«˜é¢‘è¯ç»„ (Bigrams) ---
st.divider()
st.subheader("ğŸ”— ä½ æœ€çˆ±ç”¨çš„çŸ­è¯­ (Top Phrases)")

# ç®€å•çš„ Bigram å®ç°
bigrams = []
for line in lines:
    line_words = get_words(line)
    if len(line_words) >= 2:
        for i in range(len(line_words)-1):
            bigrams.append(f"{line_words[i]} {line_words[i+1]}")

top_bigrams = Counter(bigrams).most_common(12)

cols = st.columns(4)
for i, (phrase, count) in enumerate(top_bigrams):
    with cols[i % 4]:
        st.button(f"{phrase} ({count})", key=f"bi_{i}", disabled=True)

st.caption("åŸºäº Jieba åˆ†è¯çš„äºŒå…ƒè¯ç»„ç»Ÿè®¡")
