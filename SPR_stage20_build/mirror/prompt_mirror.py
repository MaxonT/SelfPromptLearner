import streamlit as st
import pandas as pd
import jieba
import re
import json
import numpy as np
import nltk
from wordcloud import WordCloud
from collections import Counter
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
import streamlit.components.v1 as components

# --- Session State Management (Persistence) ---
if 'lang' not in st.session_state:
    # Check query params for initial language
    qp = st.query_params
    st.session_state.lang = qp.get('lang', 'en') # Default English

if 'theme' not in st.session_state:
    st.session_state.theme = 'light' # Default Light

# Persist DataFrames across reruns
if 'cached_data' not in st.session_state:
    st.session_state.cached_data = None

# --- Translations Dictionary ---
TRANSLATIONS = {
    'page_title': {
        'en': "SPR - Mind Cockpit",
        'zh': "SPR - å…¨èƒ½æ€ç»´é©¾é©¶èˆ±"
    },
    'page_caption': {
        'en': "ğŸš€ From Quantity to Quality: Visualize Your Thinking Patterns & Evolution",
        'zh': "ğŸš€ From Quantity to Quality: æ´å¯Ÿä½ çš„æ€ç»´æ¨¡å¼ä¸è¿›åŒ–è·¯å¾„"
    },
    'upload_header': {
        'en': "ğŸ“¤ Data Center",
        'zh': "ğŸ“¤ æ•°æ®ä¸­å¿ƒ"
    },
    'upload_info': {
        'en': "Support `my_prompts.json` from Chrome Extension",
        'zh': "æ”¯æŒ Chrome æ’ä»¶å¯¼å‡ºçš„ `my_prompts.json`"
    },
    'upload_label': {
        'en': "Import Data",
        'zh': "å¯¼å…¥æ•°æ®"
    },
    'settings_header': {
        'en': "âš™ï¸ Preferences",
        'zh': "âš™ï¸ åå¥½è®¾ç½®"
    },
    'filter_short': {
        'en': "Filter short prompts (<5 chars)",
        'zh': "è¿‡æ»¤çŸ­ Prompt (<5å­—)"
    },
    'privacy_header': {
        'en': "### Privacy Note",
        'zh': "### éšç§è¯´æ˜"
    },
    'privacy_caption': {
        'en': "ğŸ”’ All calculations are done locally. No data uploaded.",
        'zh': "ğŸ”’ æ‰€æœ‰è®¡ç®—å‡åœ¨æœ¬åœ°å®Œæˆï¼Œæ•°æ®ä¸ä¸Šä¼ äº‘ç«¯ã€‚"
    },
    'upload_error': {
        'en': "Data parsing failed: {}",
        'zh': "æ•°æ®è§£æå¤±è´¥: {}"
    },
    'upload_hint': {
        'en': "ğŸ‘ˆ Please upload data file on the left",
        'zh': "ğŸ‘ˆ è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ æ•°æ®æ–‡ä»¶"
    },
    'overview_header': {
        'en': "ğŸ“Š Core Metrics",
        'zh': "ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡ (Overview)"
    },
    'metric_total': {
        'en': "Total Prompts",
        'zh': "ç´¯è®¡ Prompt"
    },
    'metric_vocab': {
        'en': "Vocabulary Size",
        'zh': "æ€ç»´è¯æ±‡é‡"
    },
    'metric_avg_len': {
        'en': "Avg Length",
        'zh': "å¹³å‡é•¿åº¦"
    },
    'metric_top_word': {
        'en': "Top Keyword",
        'zh': "Top 1 å…³é”®è¯"
    },
    'tab_insight': {
        'en': "ğŸ§  Insights",
        'zh': "ğŸ§  æ€ç»´æ´å¯Ÿ"
    },
    'tab_habit': {
        'en': "ğŸ“… Habits",
        'zh': "ğŸ“… ä¹ æƒ¯è¿½è¸ª"
    },
    'tab_data': {
        'en': "ğŸ“‹ Raw Data",
        'zh': "ğŸ“‹ åŸå§‹æ•°æ®"
    },
    'radar_header': {
        'en': "ğŸ•¸ï¸ Skill Radar",
        'zh': "ğŸ•¸ï¸ æŠ€èƒ½é›·è¾¾ (Skill Radar)"
    },
    'cloud_header': {
        'en': "â˜ï¸ Word Cloud",
        'zh': "â˜ï¸ åŒè¯­æ€ç»´è¯äº‘"
    },
    'cloud_warning': {
        'en': "Not enough data for word cloud",
        'zh': "æ•°æ®é‡ä¸è¶³ä»¥ç”Ÿæˆè¯äº‘"
    },
    'dist_header': {
        'en': "ğŸ“ˆ Depth & Length Distribution",
        'zh': "ğŸ“ˆ æ·±åº¦ä¸é•¿åº¦åˆ†å¸ƒ"
    },
    'dist_len_title': {
        'en': "Prompt Length Distribution",
        'zh': "Prompt é•¿åº¦åˆ†å¸ƒ"
    },
    'dist_len_label': {
        'en': "Characters",
        'zh': "å­—ç¬¦æ•°"
    },
    'dist_comp_title': {
        'en': "Complexity Score Distribution (0-100)",
        'zh': "æ€ç»´æ·±åº¦è¯„åˆ†åˆ†å¸ƒ (0-100)"
    },
    'dist_comp_label': {
        'en': "Complexity Score",
        'zh': "å¤æ‚åº¦è¯„åˆ†"
    },
    'phrases_header': {
        'en': "ğŸ”— Top Phrases",
        'zh': "ğŸ”— ä½ æœ€çˆ±ç”¨çš„çŸ­è¯­ (Top Phrases)"
    },
    'habit_heatmap_header': {
        'en': "ğŸ”¥ Activity Heatmap (GitHub Style)",
        'zh': "ğŸ”¥ æ´»è·ƒçƒ­åŠ›å›¾ (GitHub Style)"
    },
    'trend_caption': {
        'en': "ğŸ“… Daily Activity Trend",
        'zh': "ğŸ“… æ¯æ—¥æ´»è·ƒåº¦è¶‹åŠ¿"
    },
    'hour_caption': {
        'en': "ğŸ•°ï¸ 24h Energy Distribution",
        'zh': "ğŸ•°ï¸ 24å°æ—¶ç²¾åŠ›åˆ†å¸ƒ"
    },
    'tab_bar': {
        'en': "Bar Chart",
        'zh': "æŸ±çŠ¶å›¾"
    },
    'tab_line': {
        'en': "Line Chart",
        'zh': "æŠ˜çº¿å›¾"
    },
    'hour_label': {
        'en': "Hour (0-23)",
        'zh': "å°æ—¶ (0-23)"
    },
    'count_label': {
        'en': "Count",
        'zh': "æ•°é‡"
    },
    'habit_warning': {
        'en': "âš ï¸ Missing timestamp data. Please use new extension version to export.",
        'zh': "âš ï¸ æ•°æ®ç¼ºå°‘æ—¶é—´æˆ³ï¼Œæ— æ³•æ˜¾ç¤ºä¹ æƒ¯è¿½è¸ªã€‚è¯·ä½¿ç”¨æ–°ç‰ˆæ’ä»¶å¯¼å‡º JSONã€‚"
    },
    'search_header': {
        'en': "ğŸ” Deep Search",
        'zh': "ğŸ” æ·±åº¦æœç´¢"
    },
    'search_placeholder': {
        'en': "Search keywords...",
        'zh': "æœç´¢å…³é”®è¯..."
    },
    'min_score_label': {
        'en': "Min Complexity",
        'zh': "æœ€ä½å¤æ‚åº¦"
    },
    'col_content': {
        'en': "Content",
        'zh': "å†…å®¹"
    },
    'col_score': {
        'en': "Score",
        'zh': "æ·±åº¦åˆ†"
    },
    'col_len': {
        'en': "Length",
        'zh': "é•¿åº¦"
    },
    'col_time': {
        'en': "Time",
        'zh': "æ—¶é—´"
    }
}

def t(key):
    """Get translated text based on session state"""
    return TRANSLATIONS.get(key, {}).get(st.session_state.lang, key)

# --- NLTK Setup (Fail-safe) ---
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    try:
        nltk.download('punkt', quiet=True)
    except: pass

try:
    nltk.data.find('corpora/stopwords')
    from nltk.corpus import stopwords
    english_stops = set(stopwords.words('english'))
except (LookupError, ImportError):
    english_stops = {
        "the", "a", "an", "in", "on", "at", "for", "to", "of", "is", "are", "was", "were", 
        "be", "been", "being", "have", "has", "had", "do", "does", "did", "it", "that", 
        "this", "these", "those", "i", "you", "he", "she", "we", "they", "my", "your", 
        "his", "her", "our", "their", "what", "which", "who", "whom", "whose", "where", 
        "when", "why", "how", "can", "could", "will", "would", "shall", "should", "may", 
        "might", "must", "and", "but", "or", "so", "not", "no", "yes", "please", "help", 
        "me", "thanks", "thank", "write", "create", "make", "use", "using", "code"
    }

# ä¸­æ–‡åœç”¨è¯ (De-noising)
chinese_stops = {
    "çš„", "äº†", "æ˜¯", "æˆ‘", "ä½ ", "ä»–", "åœ¨", "å’Œ", "æœ‰", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»", "èƒ½", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "æ€ä¹ˆ", "ä»€ä¹ˆ", "è¿™", "é‚£", "è¿™ä¸ª", "é‚£ä¸ª", "è¯·", "å¸®æˆ‘", "ç»™æˆ‘", "å¯ä»¥", "å—"
}

# é¡µé¢é…ç½®
st.set_page_config(page_title="SPR Mind Cockpit", layout="wide", page_icon="ğŸ§ ")

# --- å­—ä½“å¤„ç† (Mac ä¹±ç ç»ˆç»“ç‰ˆ - WordCloudç”¨) ---
import platform
def get_chinese_font():
    system = platform.system()
    if system == "Darwin": # Mac
        fonts = ["/System/Library/Fonts/PingFang.ttc", 
                 "/System/Library/Fonts/STHeiti Light.ttc",
                 "/System/Library/Fonts/Supplemental/Arial Unicode.ttf"]
        for f in fonts:
            try: 
                open(f)
                return f
            except: continue
    return None 

font_path = get_chinese_font()

# --- CSS Theme Injection (Real Dark/Light Mode) ---
# Define theme variables
themes = {
    'light': {
        'bg': '#ffffff',
        'secondary_bg': '#f0f2f6',
        'text': '#31333F',
        'card_bg': '#ffffff',
        'card_border': 'rgba(49, 51, 63, 0.1)',
        'metric_val': '#31333F'
    },
    'dark': {
        'bg': '#0e1117',
        'secondary_bg': '#262730',
        'text': '#fafafa',
        'card_bg': '#1e212b',
        'card_border': 'rgba(250, 250, 250, 0.1)',
        'metric_val': '#4cc9f0'
    }
}

current_theme = themes[st.session_state.theme]

theme_css = f"""
<style>
    :root {{
        --primary-color: #4cc9f0;
    }}
    /* Force Theme Colors */
    .stApp {{
        background-color: {current_theme['bg']};
        color: {current_theme['text']};
    }}
    
    /* Sidebar */
    [data-testid="stSidebar"] {{
        background-color: {current_theme['secondary_bg']};
    }}
    [data-testid="stSidebar"] * {{
        color: {current_theme['text']} !important;
    }}
    
    /* Metrics & Cards */
    .stMetric {{ 
        background-color: {current_theme['card_bg']} !important; 
        padding: 15px; 
        border-radius: 12px; 
        border: 1px solid {current_theme['card_border']}; 
        box-shadow: 0 4px 6px rgba(0,0,0,0.1); 
    }}
    [data-testid="stMetricValue"] {{
        color: {current_theme['metric_val']} !important;
    }}
    [data-testid="stMetricLabel"] {{
        color: {current_theme['text']} !important;
        opacity: 0.8;
    }}
    
    /* Text Colors */
    h1, h2, h3, p, span, div {{
        color: {current_theme['text']};
        font-family: 'Helvetica Neue', sans-serif;
    }}
    
    /* Input Fields */
    .stTextInput > div > div > input {{
        color: {current_theme['text']};
        background-color: {current_theme['secondary_bg']};
    }}
</style>
"""
st.markdown(theme_css, unsafe_allow_html=True)

# --- Particle Background (Cool Effect) ---
# Injects a lightweight particle.js effect
particles_html = f"""
<!DOCTYPE html>
<html>
<head>
  <style>
    #particles-js {{
      position: fixed;
      width: 100vw;
      height: 100vh;
      top: 0;
      left: 0;
      z-index: -1; /* Behind everything */
      pointer-events: none; /* Don't block clicks */
    }}
  </style>
</head>
<body>
  <div id="particles-js"></div>
  <script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
  <script>
    particlesJS("particles-js", {{
      "particles": {{
        "number": {{ "value": 60, "density": {{ "enable": true, "value_area": 800 }} }},
        "color": {{ "value": "{'#ffffff' if st.session_state.theme == 'dark' else '#4cc9f0'}" }},
        "shape": {{ "type": "circle" }},
        "opacity": {{ "value": 0.3, "random": false }},
        "size": {{ "value": 3, "random": true }},
        "line_linked": {{ "enable": true, "distance": 150, "color": "{'#ffffff' if st.session_state.theme == 'dark' else '#4cc9f0'}", "opacity": 0.2, "width": 1 }},
        "move": {{ "enable": true, "speed": 2, "direction": "none", "random": false, "straight": false, "out_mode": "out", "bounce": false }}
      }},
      "interactivity": {{
        "detect_on": "canvas",
        "events": {{ "onhover": {{ "enable": false }}, "onclick": {{ "enable": false }}, "resize": true }}
      }},
      "retina_detect": true
    }});
  </script>
</body>
</html>
"""
components.html(particles_html, height=0, width=0) # Hidden iframe but injects fixed bg

# --- Top Bar: Language & Theme Toggle ---
# Using columns to place buttons at top right
col_title, col_toggles = st.columns([5, 1])

with col_toggles:
    # Use a container to group buttons close together
    with st.container():
        # Language Toggle
        c_lang, c_theme = st.columns(2)
        with c_lang:
            if st.button("ğŸŒ " + ("CN" if st.session_state.lang == 'en' else "EN"), help="Switch Language"):
                st.session_state.lang = 'zh' if st.session_state.lang == 'en' else 'en'
                st.rerun()
        
        # Theme Toggle
        with c_theme:
            theme_icon = "ğŸŒ™" if st.session_state.theme == 'light' else "â˜€ï¸"
            if st.button(theme_icon, help="Toggle Light/Dark Mode"):
                st.session_state.theme = 'dark' if st.session_state.theme == 'light' else 'light'
                st.rerun()

with col_title:
    st.title(t('page_title'))
    st.caption(t('page_caption'))


# --- ä¾§è¾¹æ ï¼šä¸Šä¼ ä¸é…ç½® ---
with st.sidebar:
    st.header(t('upload_header'))
    st.info(t('upload_info'))
    up = st.file_uploader(t('upload_label'), type=["json", "txt", "jsonl"])
    
    st.divider()
    st.header(t('settings_header'))
    exclude_short = st.checkbox(t('filter_short'), value=True)
    
    st.markdown("---")
    st.markdown(t('privacy_header'))
    st.caption(t('privacy_caption'))

# --- æ•°æ®åŠ è½½ä¸æŒä¹…åŒ–é€»è¾‘ ---
lines = []
timestamps = []
sources = []

# Logic:
# 1. If user uploads a new file, process it and save to session_state.
# 2. If user clicks a button (rerun) but didn't change file, load from session_state.
# 3. If session_state is empty and no file, show hint.

if up:
    # New file uploaded or file still present in uploader
    # We use file content hash or name to detect change if needed, but simple re-read is fine
    try:
        content = up.read().decode('utf-8', errors='ignore')
        
        # Parse Logic
        new_lines = []
        new_timestamps = []
        new_sources = []
        
        # 1. å°è¯•è§£ææ–°ç‰ˆæ’ä»¶ JSON [{ts, text, src}, ...]
        if up.name.endswith('.json'):
            try:
                data = json.loads(content)
                if isinstance(data, list) and len(data) > 0:
                    if 'text' in data[0]: 
                        for item in data:
                            text = item.get('text', '')
                            if exclude_short and len(text) < 5: continue
                            new_lines.append(text)
                            ts = item.get('ts', 0)
                            if ts > 0: new_timestamps.append(datetime.fromtimestamp(ts / 1000))
                            new_sources.append(item.get('src', 'unknown'))
                    elif 'mapping' in data[0]:
                         for conv in data:
                            if 'mapping' in conv:
                                for k, v in conv['mapping'].items():
                                    if v['message'] and v['message']['author']['role'] == 'user':
                                        parts = v['message']['content']['parts']
                                        if parts: 
                                            text = str(parts[0])
                                            if exclude_short and len(text) < 5: continue
                                            new_lines.append(text)
                                            ct = v['message'].get('create_time')
                                            if ct: new_timestamps.append(datetime.fromtimestamp(ct))
            except json.JSONDecodeError:
                pass 

        # 2. å…¼å®¹æ—§ç‰ˆ TXT / JSONL
        if not new_lines: 
             if up.name.endswith('.jsonl'):
                for line in content.splitlines():
                    if line.strip():
                        try:
                            msg = json.loads(line)
                            if 'messages' in msg: new_lines.append(msg['messages'][0]['content'])
                        except: pass
             else:
                new_lines = [l.strip() for l in content.split('===SPLIT===') if l.strip()]
                if len(new_lines) < 2:
                     new_lines = [l.strip() for l in content.splitlines() if l.strip()]

        # Update Session State
        if new_lines:
            st.session_state.cached_data = {
                'lines': new_lines,
                'timestamps': new_timestamps,
                'sources': new_sources
            }
            
    except Exception as e:
        st.error(t('upload_error').format(e))

# Load from Cache if available
if st.session_state.cached_data:
    lines = st.session_state.cached_data['lines']
    timestamps = st.session_state.cached_data['timestamps']
    sources = st.session_state.cached_data['sources']

if not lines:
    st.info(t('upload_hint'))
    st.stop()

# --- æ•°æ®é¢„å¤„ç† ---
df = pd.DataFrame({"prompt": lines})
df["len"] = df["prompt"].str.len()

if timestamps and len(timestamps) == len(lines):
    df["time"] = timestamps
    df["hour"] = df["time"].dt.hour
    df["date"] = df["time"].dt.date
    df["weekday"] = df["time"].dt.weekday  # 0=Monday
    df["week_name"] = df["time"].dt.day_name()
    has_time = True
else:
    has_time = False

# --- æ ¸å¿ƒç®—æ³•ï¼šå¤æ‚åº¦è¯„åˆ† (Complexity Score) ---
def calculate_complexity(text):
    """
    è®¡ç®— Prompt çš„æ€ç»´å¤æ‚åº¦ (0-100)
    """
    score = 0
    score += min(len(text) / 200, 1.0) * 40
    logical_words = [
        'if', 'because', 'however', 'therefore', 'although', 'compare', 'difference',
        'å¦‚æœ', 'å› ä¸º', 'ä½†æ˜¯', 'æ‰€ä»¥', 'è™½ç„¶', 'æ¯”è¾ƒ', 'åŒºåˆ«', 'åŸç†', 'åˆ†æ', 'why', 'how'
    ]
    logic_hits = sum(1 for w in logical_words if w in text.lower())
    score += min(logic_hits / 3, 1.0) * 30
    if '```' in text or '\n-' in text or '\n1.' in text:
        score += 30
    return min(int(score), 100)

df['complexity'] = df['prompt'].apply(calculate_complexity)

# --- æ ¸å¿ƒç®—æ³•ï¼šåŒè¯­åˆ†è¯ (Bilingual NLP + De-noising) ---
@st.cache_data
def process_tokens(text_list):
    all_text = " ".join(text_list)
    
    # 1. ä¸­æ–‡åˆ†è¯ (jieba) + å»åœç”¨è¯
    zh_pattern = re.compile(r'[\u4e00-\u9fa5]+')
    zh_words = [w for w in jieba.lcut(all_text) if len(w) > 1 and zh_pattern.match(w) and w not in chinese_stops]
    
    # 2. è‹±æ–‡åˆ†è¯ (ç®€å•æ­£åˆ™ + NLTK Stopwords)
    en_pattern = re.compile(r'[a-zA-Z]{2,}')
    en_words = en_pattern.findall(all_text.lower())
    en_words = [w for w in en_words if w not in english_stops]
    
    return zh_words + en_words

words = process_tokens(lines)
word_counts = Counter(words)

# --- Dashboard æ¦‚è§ˆ ---
st.subheader(t('overview_header'))
c1, c2, c3, c4 = st.columns(4)
c1.metric(t('metric_total'), f"{len(df)}", delta=f"Avg Complexity: {int(df['complexity'].mean())}")
c2.metric(t('metric_vocab'), f"{len(word_counts)}")
c3.metric(t('metric_avg_len'), f"{int(df['len'].mean())}")
top_word = word_counts.most_common(1)[0][0] if word_counts else "N/A"
c4.metric(t('metric_top_word'), top_word)

st.divider()

# --- Tab å¸ƒå±€ ---
tab_insight, tab_habit, tab_data = st.tabs([t('tab_insight'), t('tab_habit'), t('tab_data')])

# === Tab 1: æ€ç»´æ´å¯Ÿ ===
with tab_insight:
    col_radar, col_cloud = st.columns([1, 1.5])
    
    # Advanced Plotly Template (Cyberpunk Style)
    chart_template = "plotly_dark" if st.session_state.theme == 'dark' else "plotly_white"
    accent_color = "#4cc9f0" if st.session_state.theme == 'dark' else "#3a86ff"
    
    with col_radar:
        st.subheader(t('radar_header'))
        
        category_defs = {
            "coding": {
                "keywords": ["ä»£ç ", "code", "å‡½æ•°", "æŠ¥é”™", "bug", "python", "js", "react", "sql", "api", "å†™ä¸€ä¸ª", "å®ç°", "function", "class", "error", "æ¥å£"],
                "label_en": "Coding",
                "label_zh": "ğŸ’» ç¼–ç¨‹å¼€å‘"
            },
            "writing": {
                "keywords": ["æ–‡æ¡ˆ", "æ–‡ç« ", "å‘¨æŠ¥", "æ€»ç»“", "æ‰©å†™", "æ¶¦è‰²", "å¤§çº²", "æ ‡é¢˜", "ç¿»è¯‘", "é‚®ä»¶", "write", "email", "article", "summary", "translate", "outline", "title"],
                "label_en": "Writing",
                "label_zh": "ğŸ“ å†…å®¹åˆ›ä½œ"
            },
            "logic": {
                "keywords": ["åˆ†æ", "åŸå› ", "åŒºåˆ«", "æ¯”è¾ƒ", "è¯„ä»·", "ä¼˜ç¼ºç‚¹", "å»ºè®®", "æ–¹æ¡ˆ", "æ€ç»´å¯¼å›¾", "analyze", "reason", "compare", "difference", "pros", "cons", "plan"],
                "label_en": "Logic",
                "label_zh": "ğŸ§  é€»è¾‘åˆ†æ"
            },
            "learning": {
                "keywords": ["è§£é‡Š", "ä»‹ç»", "æ˜¯ä»€ä¹ˆ", "å«ä¹‰", "åŸç†", "æ•™ç¨‹", "å­¦ä¹ ", "å¦‚ä½•", "explain", "what", "how", "meaning", "tutorial", "principle", "learn"],
                "label_en": "Learning",
                "label_zh": "ğŸ“ çŸ¥è¯†å­¦ä¹ "
            },
            "creative": {
                "keywords": ["åˆ›æ„", "ç‚¹å­", "æ•…äº‹", "è®¾æƒ³", "å¦‚æœ", "ç”Ÿæˆ", "è®¾è®¡", "idea", "story", "design", "imagine", "generate", "create"],
                "label_en": "Creative",
                "label_zh": "ğŸ¨ åˆ›æ„è„‘æš´"
            }
        }
        
        cat_scores = {k: 0 for k in category_defs.keys()}
        for w in words:
            for cat_key, cat_data in category_defs.items():
                if w in cat_data['keywords']:
                    cat_scores[cat_key] += 1
        
        vals = list(cat_scores.values())
        max_val = max(vals) if vals else 1
        normalized_vals = [v/max_val for v in vals]
        
        labels = [category_defs[k][f'label_{st.session_state.lang}'] for k in category_defs.keys()]
        
        fig_radar = px.line_polar(r=normalized_vals, theta=labels, line_close=True, template=chart_template)
        fig_radar.update_traces(fill='toself', line_color=accent_color)
        fig_radar.update_layout(
            polar=dict(radialaxis=dict(visible=False)),
            margin=dict(t=20, b=20, l=30, r=30),
            height=350,
            paper_bgcolor='rgba(0,0,0,0)',
            plot_bgcolor='rgba(0,0,0,0)'
        )
        st.plotly_chart(fig_radar, use_container_width=True)

    with col_cloud:
        st.subheader(t('cloud_header'))
        if words:
            wc = WordCloud(font_path=font_path, width=800, height=500, 
                          background_color=None, mode="RGBA", # é€æ˜èƒŒæ™¯
                          max_words=100, collocations=False).generate(" ".join(words))
            st.image(wc.to_array(), use_column_width=True)
        else:
            st.warning(t('cloud_warning'))

    st.subheader(t('dist_header'))
    c_len, c_comp = st.columns(2)
    
    with c_len:
        fig_len = px.histogram(
             df, x="len", nbins=30,
             title=t('dist_len_title'),
             labels={'len': t('dist_len_label'), 'count': t('count_label')},
             color_discrete_sequence=['#ffbe0b'],
             template=chart_template
        )
        fig_len.update_layout(showlegend=False, paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)')
        st.plotly_chart(fig_len, use_container_width=True)

    with c_comp:
        fig_comp = px.histogram(
            df, x="complexity", nbins=20, 
            title=t('dist_comp_title'),
            labels={'complexity': t('dist_comp_label'), 'count': t('count_label')},
            color_discrete_sequence=['#7209b7'],
            template=chart_template
        )
        fig_comp.update_layout(bargap=0.1, paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)')
        st.plotly_chart(fig_comp, use_container_width=True)

    # æ¢å¤é«˜é¢‘è¯ç»„ (Bigrams) æ¿å—
    st.divider()
    st.subheader(t('phrases_header'))
    
    bigrams = []
    for line in lines:
        # ç®€å•åˆ†è¯ç”¨äº bigram
        line_words = [w for w in jieba.lcut(line) if len(w) > 1 and re.match(r"[\u4e00-\u9fa5a-zA-Z]", w)]
        if len(line_words) >= 2:
            for i in range(len(line_words)-1):
                bigrams.append(f"{line_words[i]} {line_words[i+1]}")

    top_bigrams = Counter(bigrams).most_common(12)

    cols = st.columns(4)
    for i, (phrase, count) in enumerate(top_bigrams):
        with cols[i % 4]:
            st.button(f"{phrase} ({count})", key=f"bi_{i}", disabled=True)


# === Tab 2: ä¹ æƒ¯è¿½è¸ª ===
with tab_habit:
    if has_time:
        st.subheader(t('habit_heatmap_header'))
        
        daily_counts = df['date'].value_counts().reset_index()
        daily_counts.columns = ['date', 'count']
        daily_counts['date'] = pd.to_datetime(daily_counts['date'])
        
        c1, c2 = st.columns([2, 1])
        
        with c1:
            st.caption(t('trend_caption'))
            fig_trend = px.bar(daily_counts.sort_values('date'), x='date', y='count', 
                              color='count', color_continuous_scale='Blues', template=chart_template)
            fig_trend.update_layout(paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)')
            st.plotly_chart(fig_trend, use_container_width=True)
            
        with c2:
            st.caption(t('hour_caption'))
            hour_counts = df['hour'].value_counts().sort_index().reset_index()
            hour_counts.columns = ['hour', 'count']
            
            tab_bar, tab_line = st.tabs([t('tab_bar'), t('tab_line')])
            
            with tab_bar:
                fig_bar = px.bar(
                    hour_counts, x='hour', y='count',
                    labels={'hour': t('hour_label'), 'count': t('count_label')},
                    template=chart_template
                )
                fig_bar.update_traces(marker_color=accent_color)
                fig_bar.update_layout(xaxis=dict(tickmode='linear', dtick=2), paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)')
                st.plotly_chart(fig_bar, use_container_width=True)
                
            with tab_line:
                fig_hour = px.line(hour_counts, x='hour', y='count', markers=True, template=chart_template)
                fig_hour.update_traces(line_color='#f72585')
                fig_hour.update_layout(paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)')
                st.plotly_chart(fig_hour, use_container_width=True)
            
    else:
        st.warning(t('habit_warning'))

# === Tab 3: åŸå§‹æ•°æ® ===
with tab_data:
    st.subheader(t('search_header'))
    
    col_search, col_filter = st.columns([3, 1])
    with col_search:
        q = st.text_input(t('search_placeholder'), placeholder="Python, Writing...")
    with col_filter:
        min_score = st.slider(t('min_score_label'), 0, 100, 0)
    
    filtered_df = df.copy()
    if q:
        filtered_df = filtered_df[filtered_df['prompt'].str.contains(q, case=False)]
    filtered_df = filtered_df[filtered_df['complexity'] >= min_score]
    
    if has_time:
        filtered_df['time_str'] = filtered_df['time'].dt.strftime('%Y-%m-%d %H:%M')
        show_cols = ['time_str', 'prompt', 'complexity', 'len']
    else:
        show_cols = ['prompt', 'complexity', 'len']
        
    st.dataframe(
        filtered_df[show_cols].sort_values('complexity', ascending=False),
        column_config={
            "prompt": st.column_config.TextColumn(t('col_content'), width="large"),
            "complexity": st.column_config.ProgressColumn(t('col_score'), format="%d", min_value=0, max_value=100),
            "len": st.column_config.NumberColumn(t('col_len')),
            "time_str": st.column_config.TextColumn(t('col_time'))
        },
        use_container_width=True,
        height=600,
        hide_index=True
    )
