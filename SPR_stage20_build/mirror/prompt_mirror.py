import streamlit as st
import pandas as pd
import jieba
import re
import json
import numpy as np
import nltk
from wordcloud import WordCloud
from collections import Counter
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
import streamlit.components.v1 as components
import textwrap

# --- Session State Management (Persistence) ---
if 'lang' not in st.session_state:
    # Check query params for initial language
    qp = st.query_params
    st.session_state.lang = qp.get('lang', 'en') # Default English

# FORCE LUXURY DARK MODE (Remove 'theme' toggle logic)
st.session_state.theme = 'dark'

# Persist DataFrames across reruns
if 'cached_data' not in st.session_state:
    st.session_state.cached_data = None

# --- Translations Dictionary ---
TRANSLATIONS = {
    'page_title': {
        'en': "SPR - Mind Cockpit",
        'zh': "SPR - å…¨èƒ½æ€ç»´é©¾é©¶èˆ±"
    },
    'page_caption': {
        'en': "ğŸš€ From Quantity to Quality: Visualize Your Thinking Patterns & Evolution",
        'zh': "ğŸš€ From Quantity to Quality: æ´å¯Ÿä½ çš„æ€ç»´æ¨¡å¼ä¸è¿›åŒ–è·¯å¾„"
    },
    'upload_header': {
        'en': "ğŸ“¤ Data Center",
        'zh': "ğŸ“¤ æ•°æ®ä¸­å¿ƒ"
    },
    'upload_info': {
        'en': "Support `my_prompts.json` from Chrome Extension",
        'zh': "æ”¯æŒ Chrome æ’ä»¶å¯¼å‡ºçš„ `my_prompts.json`"
    },
    'upload_label': {
        'en': "Import Data",
        'zh': "å¯¼å…¥æ•°æ®"
    },
    'settings_header': {
        'en': "âš™ï¸ Preferences",
        'zh': "âš™ï¸ åå¥½è®¾ç½®"
    },
    'filter_short': {
        'en': "Filter short prompts (<5 chars)",
        'zh': "è¿‡æ»¤çŸ­ Prompt (<5å­—)"
    },
    'privacy_header': {
        'en': "### Privacy Note",
        'zh': "### éšç§è¯´æ˜"
    },
    'privacy_caption': {
        'en': "ğŸ”’ All calculations are done locally. No data uploaded.",
        'zh': "ğŸ”’ æ‰€æœ‰è®¡ç®—å‡åœ¨æœ¬åœ°å®Œæˆï¼Œæ•°æ®ä¸ä¸Šä¼ äº‘ç«¯ã€‚"
    },
    'upload_error': {
        'en': "Data parsing failed: {}",
        'zh': "æ•°æ®è§£æå¤±è´¥: {}"
    },
    'upload_hint': {
        'en': "ğŸ‘ˆ Please upload data file on the left",
        'zh': "ğŸ‘ˆ è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ æ•°æ®æ–‡ä»¶"
    },
    'tab_manage': {
        'en': "ğŸ—‚ï¸ Data Manager",
        'zh': "ğŸ—‚ï¸ æ•°æ®ç®¡ç†"
    },
    'manage_header': {
        'en': "Data Management",
        'zh': "æ•°æ®ç®¡ç†ä¸­å¿ƒ"
    },
    'delete_btn': {
        'en': "Delete Selected",
        'zh': "åˆ é™¤é€‰ä¸­é¡¹"
    },
    'privacy_title': {
        'en': "ğŸ”’ Privacy & Legal",
        'zh': "ğŸ”’ éšç§ä¸æ³•å¾‹å£°æ˜"
    },
    'privacy_content': {
        'en': """
        **Local Processing Only**
        Your data never leaves your browser/local machine. All analysis is performed locally using Python.
        
        **Open Source**
        This tool is open source under MIT License. You have full control over your data.
        """,
        'zh': """
        **çº¯æœ¬åœ°å¤„ç†**
        æ‚¨çš„æ•°æ®æ°¸è¿œä¸ä¼šç¦»å¼€æ‚¨çš„æµè§ˆå™¨æˆ–æœ¬åœ°æœºå™¨ã€‚æ‰€æœ‰åˆ†æå‡ä½¿ç”¨ Python åœ¨æœ¬åœ°å®Œæˆã€‚
        
        **å¼€æºé€æ˜**
        æœ¬å·¥å…·åŸºäº MIT åè®®å¼€æºã€‚æ‚¨æ‹¥æœ‰å¯¹æ•°æ®çš„å®Œå…¨æ§åˆ¶æƒã€‚
        """
    },
    'filter_strict': {
        'en': "Strict Filter (Remove generic/junk)",
        'zh': "ä¸¥æ ¼è¿‡æ»¤ (ç§»é™¤é€šç”¨/åƒåœ¾ Prompt)"
    },
    'overview_header': {
        'en': "ğŸ“Š Core Metrics",
        'zh': "ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡ (Overview)"
    },
    'metric_total': {
        'en': "Total Prompts",
        'zh': "ç´¯è®¡ Prompt"
    },
    'metric_vocab': {
        'en': "Vocabulary Size",
        'zh': "æ€ç»´è¯æ±‡é‡"
    },
    'metric_avg_len': {
        'en': "Avg Length",
        'zh': "å¹³å‡é•¿åº¦"
    },
    'metric_top_word': {
        'en': "Top Keyword",
        'zh': "Top 1 å…³é”®è¯"
    },
    'tab_insight': {
        'en': "ğŸ§  Insights",
        'zh': "ğŸ§  æ€ç»´æ´å¯Ÿ"
    },
    'tab_habit': {
        'en': "ğŸ“… Habits",
        'zh': "ğŸ“… ä¹ æƒ¯è¿½è¸ª"
    },
    'tab_data': {
        'en': "ğŸ“‹ Raw Data",
        'zh': "ğŸ“‹ åŸå§‹æ•°æ®"
    },
    'radar_header': {
        'en': "ğŸ•¸ï¸ Skill Radar",
        'zh': "ğŸ•¸ï¸ æŠ€èƒ½é›·è¾¾ (Skill Radar)"
    },
    'cloud_header': {
        'en': "â˜ï¸ Word Cloud",
        'zh': "â˜ï¸ åŒè¯­æ€ç»´è¯äº‘"
    },
    'cloud_warning': {
        'en': "Not enough data for word cloud",
        'zh': "æ•°æ®é‡ä¸è¶³ä»¥ç”Ÿæˆè¯äº‘"
    },
    'dist_header': {
        'en': "ğŸ“ˆ Depth & Length Distribution",
        'zh': "ğŸ“ˆ æ·±åº¦ä¸é•¿åº¦åˆ†å¸ƒ"
    },
    'dist_len_title': {
        'en': "Prompt Length Distribution",
        'zh': "Prompt é•¿åº¦åˆ†å¸ƒ"
    },
    'dist_len_label': {
        'en': "Characters",
        'zh': "å­—ç¬¦æ•°"
    },
    'dist_comp_title': {
        'en': "Complexity Score Distribution (0-100)",
        'zh': "æ€ç»´æ·±åº¦è¯„åˆ†åˆ†å¸ƒ (0-100)"
    },
    'dist_comp_label': {
        'en': "Complexity Score",
        'zh': "å¤æ‚åº¦è¯„åˆ†"
    },
    'phrases_header': {
        'en': "ğŸ”— Top Phrases",
        'zh': "ğŸ”— ä½ æœ€çˆ±ç”¨çš„çŸ­è¯­ (Top Phrases)"
    },
    'habit_heatmap_header': {
        'en': "ğŸ”¥ Activity Heatmap (GitHub Style)",
        'zh': "ğŸ”¥ æ´»è·ƒçƒ­åŠ›å›¾ (GitHub Style)"
    },
    'trend_caption': {
        'en': "ğŸ“… Daily Activity Trend",
        'zh': "ğŸ“… æ¯æ—¥æ´»è·ƒåº¦è¶‹åŠ¿"
    },
    'hour_caption': {
        'en': "ğŸ•°ï¸ 24h Energy Distribution",
        'zh': "ğŸ•°ï¸ 24å°æ—¶ç²¾åŠ›åˆ†å¸ƒ"
    },
    'tab_bar': {
        'en': "Bar Chart",
        'zh': "æŸ±çŠ¶å›¾"
    },
    'tab_line': {
        'en': "Line Chart",
        'zh': "æŠ˜çº¿å›¾"
    },
    'hour_label': {
        'en': "Hour (0-23)",
        'zh': "å°æ—¶ (0-23)"
    },
    'count_label': {
        'en': "Count",
        'zh': "æ•°é‡"
    },
    'habit_warning': {
        'en': "âš ï¸ Missing timestamp data. Please use new extension version to export.",
        'zh': "âš ï¸ æ•°æ®ç¼ºå°‘æ—¶é—´æˆ³ï¼Œæ— æ³•æ˜¾ç¤ºä¹ æƒ¯è¿½è¸ªã€‚è¯·ä½¿ç”¨æ–°ç‰ˆæ’ä»¶å¯¼å‡º JSONã€‚"
    },
    'search_header': {
        'en': "ğŸ” Deep Search",
        'zh': "ğŸ” æ·±åº¦æœç´¢"
    },
    'search_placeholder': {
        'en': "Search keywords...",
        'zh': "æœç´¢å…³é”®è¯..."
    },
    'min_score_label': {
        'en': "Min Complexity",
        'zh': "æœ€ä½å¤æ‚åº¦"
    },
    'col_content': {
        'en': "Content",
        'zh': "å†…å®¹"
    },
    'col_score': {
        'en': "Score",
        'zh': "æ·±åº¦åˆ†"
    },
    'col_len': {
        'en': "Length",
        'zh': "é•¿åº¦"
    },
    'col_time': {
        'en': "Time",
        'zh': "æ—¶é—´"
    }
}

def t(key):
    """Get translated text based on session state"""
    return TRANSLATIONS.get(key, {}).get(st.session_state.lang, key)

# --- NLTK Setup (Fail-safe) ---
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    try:
        nltk.download('punkt', quiet=True)
    except: pass

try:
    nltk.data.find('corpora/stopwords')
    from nltk.corpus import stopwords
    english_stops = set(stopwords.words('english'))
except (LookupError, ImportError):
    english_stops = set()

# Enhanced Stopwords (De-noising)
english_stops.update({
    "the", "a", "an", "in", "on", "at", "for", "to", "of", "is", "are", "was", "were", 
    "be", "been", "being", "have", "has", "had", "do", "does", "did", "it", "that", 
    "this", "these", "those", "i", "you", "he", "she", "we", "they", "my", "your", 
    "his", "her", "our", "their", "what", "which", "who", "whom", "whose", "where", 
    "when", "why", "how", "can", "could", "will", "would", "shall", "should", "may", 
    "might", "must", "and", "but", "or", "so", "not", "no", "yes", "please", "help", 
    "me", "thanks", "thank", "write", "create", "make", "use", "using", "code",
    "want", "need", "like", "just", "get", "go", "know", "think", "see", "say",
    "tell", "ask", "try", "look", "take", "give", "find", "use", "way", "new",
    "good", "great", "well", "much", "many", "lot", "little", "big", "small",
    # Prompt Engineering Boilerplate (Noise for WordCloud)
    "act", "role", "assume", "step", "context", "instruction", "output", "format",
    "generate", "rewrite", "revise", "optimize", "check", "verify", "explain",
    "translation", "translate", "english", "chinese", "text", "sentence", "paragraph",
    "based", "following", "provided", "below", "above", "result", "answer", "question",
    "style", "tone", "ensure", "make", "sure", "include", "example", "list"
})

# ä¸­æ–‡åœç”¨è¯ (De-noising)
chinese_stops = {
    "çš„", "äº†", "æ˜¯", "æˆ‘", "ä½ ", "ä»–", "åœ¨", "å’Œ", "æœ‰", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»", "èƒ½", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "æ€ä¹ˆ", "ä»€ä¹ˆ", "è¿™", "é‚£", "è¿™ä¸ª", "é‚£ä¸ª", "è¯·", "å¸®æˆ‘", "ç»™æˆ‘", "å¯ä»¥", "å—",
    "ä¸ª", "åª", "æ¬¡", "æŠŠ", "è¢«", "è®©", "ç»™", "ä½†", "å› ä¸º", "æ‰€ä»¥", "å¦‚æœ", "è™½ç„¶", "ä½†æ˜¯", "æˆ–è€…", "è¿˜æ˜¯", "ä»¥åŠ", "é™¤äº†", "ä¸ºäº†", "å…³äº", "å¯¹äº", "é€šè¿‡", "æ ¹æ®", "æŒ‰ç…§", "ä½œä¸º", "éšç€",
    # Prompt Engineering Boilerplate (Chinese)
    "æ‰®æ¼”", "è§’è‰²", "ç”Ÿæˆ", "è¾“å‡º", "æ ¼å¼", "è¦æ±‚", "ä¸Šä¸‹æ–‡", "æ­¥éª¤", "è§£é‡Š", "ç¿»è¯‘",
    "è‹±æ–‡", "ä¸­æ–‡", "ä»£ç ", "æ–‡ç« ", "å†…å®¹", "ä»¥ä¸‹", "ä»¥ä¸Š", "æä¾›", "åŸºäº", "ä½¿ç”¨",
    "ä¸ä»…", "è€Œä¸”", "èƒ½å¤Ÿ", "éœ€è¦", "å¸®å¿™", "ä¿®æ”¹", "æ¶¦è‰²", "ä¼˜åŒ–", "æ£€æŸ¥", "å†™ä¸€ä¸ª",
    "å¸®æˆ‘å†™", "å¸®æˆ‘çœ‹", "æ€ä¹ˆå†™", "æ€ä¹ˆåš"
}

# é¡µé¢é…ç½®
st.set_page_config(page_title="SPR Mind Cockpit", layout="wide", page_icon="ğŸ§ ")

# --- å­—ä½“å¤„ç† (Mac ä¹±ç ç»ˆç»“ç‰ˆ - WordCloudç”¨) ---
import platform
def get_chinese_font():
    system = platform.system()
    if system == "Darwin": # Mac
        fonts = [
            "/System/Library/Fonts/PingFang.ttc", 
            "/System/Library/Fonts/STHeiti Light.ttc",
            "/System/Library/Fonts/Hiragino Sans GB.ttc",
            "/System/Library/Fonts/Supplemental/Arial Unicode.ttf",
            "/Library/Fonts/Arial Unicode.ttf"
        ]
        for f in fonts:
            try: 
                open(f)
                return f
            except: continue
    return None 

font_path = get_chinese_font()

# --- Luxury CSS Injection ---
luxury_css = """
<style>
    /* Global Background: Deep Blue-Black Gradient */
    .stApp {
        background: radial-gradient(circle at 50% 10%, #1e293b 0%, #0f172a 40%, #020617 100%) !important;
        color: #e2e8f0;
    }

    /* Noise Texture Overlay */
    .stApp::before {
        content: "";
        position: fixed;
        top: 0;
        left: 0;
        width: 100vw;
        height: 100vh;
        background-image: url("data:image/svg+xml,%3Csvg viewBox='0 0 200 200' xmlns='http://www.w3.org/2000/svg'%3E%3Cfilter id='noiseFilter'%3E%3CfeTurbulence type='fractalNoise' baseFrequency='0.65' numOctaves='3' stitchTiles='stitch'/%3E%3C/filter%3E%3Crect width='100%25' height='100%25' filter='url(%23noiseFilter)' opacity='0.03'/%3E%3C/svg%3E");
        pointer-events: none;
        z-index: 0;
    }

    /* Liquid Glass Cards */
    .stMetric, .stDataFrame, .stPlotlyChart {
        background: rgba(255, 255, 255, 0.03) !important;
        backdrop-filter: blur(12px);
        -webkit-backdrop-filter: blur(12px);
        border: 1px solid rgba(255, 255, 255, 0.08) !important;
        border-radius: 16px !important;
        box-shadow: 0 8px 32px 0 rgba(0, 0, 0, 0.2);
        padding: 16px !important;
        transition: all 0.3s ease;
    }

    .stMetric:hover, .stDataFrame:hover {
        background: rgba(255, 255, 255, 0.05) !important;
        border: 1px solid rgba(255, 255, 255, 0.15) !important;
        transform: translateY(-2px);
        box-shadow: 0 12px 40px 0 rgba(0, 0, 0, 0.3);
    }

    /* Metric Values (Champagne Gold) */
    [data-testid="stMetricValue"] {
        color: #D4AF37 !important;
        font-family: 'Helvetica Neue', sans-serif;
        font-weight: 300;
        text-shadow: 0 0 20px rgba(212, 175, 55, 0.3);
    }
    
    /* Metric Labels (Soft White) */
    [data-testid="stMetricLabel"] {
        color: rgba(255, 255, 255, 0.7) !important;
        font-size: 0.9em;
        letter-spacing: 0.5px;
    }

    /* Sidebar Luxury */
    [data-testid="stSidebar"] {
        background: rgba(15, 23, 42, 0.95) !important;
        border-right: 1px solid rgba(255, 255, 255, 0.05);
    }
    
    /* Buttons (Glassy) */
    .stButton > button {
        background: rgba(255, 255, 255, 0.05) !important;
        color: #D4AF37 !important;
        border: 1px solid rgba(212, 175, 55, 0.3) !important;
        border-radius: 8px;
        transition: all 0.3s;
    }
    .stButton > button:hover {
        background: rgba(212, 175, 55, 0.1) !important;
        border-color: #D4AF37 !important;
        box-shadow: 0 0 15px rgba(212, 175, 55, 0.2);
    }

    /* Titles */
    h1, h2, h3 {
        color: #f8fafc !important;
        font-weight: 300 !important;
        letter-spacing: -0.5px;
    }


    /* File Uploader Enhancement (High Visibility) */
    [data-testid="stFileUploader"] {
        background: rgba(255, 255, 255, 0.05);
        border: 2px dashed rgba(255, 255, 255, 0.3);
        border-radius: 16px;
        padding: 30px;
        transition: all 0.3s ease;
        text-align: center;
    }
    [data-testid="stFileUploader"]:hover {
        background: rgba(255, 255, 255, 0.08);
        border-color: #D4AF37;
        box-shadow: 0 0 20px rgba(212, 175, 55, 0.2);
        transform: scale(1.02);
    }
    [data-testid="stFileUploader"] section {
        background: transparent !important;
    }
    [data-testid="stFileUploader"] button {
        background: rgba(212, 175, 55, 0.2) !important;
        color: #D4AF37 !important;
        border: 1px solid #D4AF37 !important;
        font-weight: bold;
    }
</style>
"""
st.markdown(luxury_css, unsafe_allow_html=True)


# --- Top Bar: Language Toggle Only ---
col_title, col_lang = st.columns([6, 1])

with col_lang:
    if st.button("ğŸŒ " + ("CN" if st.session_state.lang == 'en' else "EN"), help="Switch Language"):
        st.session_state.lang = 'zh' if st.session_state.lang == 'en' else 'en'
        st.rerun()

with col_title:
    st.title(t('page_title'))
    st.caption(t('page_caption'))


# --- ä¾§è¾¹æ ï¼šä¸Šä¼ ä¸é…ç½® ---
with st.sidebar:
    st.header(t('upload_header'))
    st.info(t('upload_info'))
    up = st.file_uploader(t('upload_label'), type=["json", "txt", "jsonl"])
    
    st.divider()
    st.header(t('settings_header'))
    exclude_short = st.checkbox(t('filter_short'), value=True)
    strict_filter = st.checkbox(t('filter_strict'), value=False)
    
    st.markdown("---")
    with st.expander(t('privacy_title')):
        st.markdown(t('privacy_content'))

# --- å…³é”® CSS åŠ¨ç”»å®šä¹‰ ---
st.markdown("""
<style>
@keyframes bounce-left {
  0%, 100% { transform: translateX(0); }
  50% { transform: translateX(-10px); }
}
@keyframes bounce-up-right {
  0%, 100% { transform: translate(0, 0); }
  50% { transform: translate(10px, -10px); }
}
.animate-arrow {
  animation: bounce-left 1.5s infinite ease-in-out;
  display: inline-block;
}
.animate-arrow-ur {
  animation: bounce-up-right 1.5s infinite ease-in-out;
  display: inline-block;
}
</style>
""", unsafe_allow_html=True)

# --- Empty State (Onboarding Guide) ---
def show_onboarding_guide():
    st.markdown("<br><br>", unsafe_allow_html=True)
    c1, c2, c3 = st.columns([1, 2, 1])
    with c2:
        # Flattened HTML to prevent Markdown code block parsing
        html_content = """
<div style="text-align: center; padding: 40px; background: rgba(255,255,255,0.03); border-radius: 20px; border: 1px solid rgba(255,255,255,0.1);">
<div style="font-size: 60px; margin-bottom: 20px;">ğŸš€</div>
<h2 style="color: #D4AF37;">Welcome to Mind Cockpit</h2>
<p style="color: rgba(255,255,255,0.7); font-size: 18px; margin-bottom: 30px;">Your personal thinking analytics dashboard is ready.</p>
<div style="display: flex; align-items: center; justify-content: center; gap: 20px; margin-top: 40px;">
<div class="animate-arrow-ur" style="font-size: 40px; color: #4cc9f0;">â†—ï¸</div>
<div style="text-align: left;">
<div style="font-weight: bold; color: #fff; font-size: 20px;">Step 1</div>
<div style="color: rgba(255,255,255,0.6);">Click Extension in top-right to export <code>my_prompts.json</code></div>
</div>
</div>
<div style="display: flex; align-items: center; justify-content: center; gap: 20px; margin-top: 20px;">
<div class="animate-arrow" style="font-size: 40px; color: #4cc9f0;">ğŸ‘ˆ</div>
<div style="text-align: left;">
<div style="font-weight: bold; color: #fff; font-size: 20px;">Step 2</div>
<div style="color: rgba(255,255,255,0.6);">Drag & Drop file to the sidebar</div>
</div>
</div>
</div>
"""
        st.markdown(html_content, unsafe_allow_html=True)

# --- æ•°æ®åŠ è½½ä¸æŒä¹…åŒ–é€»è¾‘ ---
lines = []
timestamps = []
sources = []

if up:
    try:
        content = up.read().decode('utf-8', errors='ignore')
        new_lines = []
        new_timestamps = []
        new_sources = []
        
        if up.name.endswith('.json'):
            try:
                data = json.loads(content)
                if isinstance(data, list) and len(data) > 0:
                    if 'text' in data[0]: 
                        for item in data:
                            text = item.get('text', '')
                            # Junk Filter: Remove pure punctuation/numbers or very short generic words
                            if exclude_short and len(text) < 5: continue
                            if re.match(r'^[\s\d\W]+$', text): continue # Only symbols/numbers
                            
                            # Strict Filter
                            if strict_filter:
                                if len(text) < 10: continue
                                if any(w in text.lower() for w in ["test", "hello", "hi", "ä½ å¥½", "æµ‹è¯•", "demo"]): continue
                            
                            if text.lower().strip() in ["hi", "hello", "test", "testing", "ä½ å¥½", "æµ‹è¯•"]: continue
                            
                            new_lines.append(text)
                            ts = item.get('ts', 0)
                            if ts > 0: new_timestamps.append(datetime.fromtimestamp(ts / 1000))
                            new_sources.append(item.get('src', 'unknown'))
                    elif 'mapping' in data[0]:
                         for conv in data:
                            if 'mapping' in conv:
                                for k, v in conv['mapping'].items():
                                    if v['message'] and v['message']['author']['role'] == 'user':
                                        parts = v['message']['content']['parts']
                                        if parts: 
                                            text = str(parts[0])
                                            if exclude_short and len(text) < 5: continue
                                            new_lines.append(text)
                                            ct = v['message'].get('create_time')
                                            if ct: new_timestamps.append(datetime.fromtimestamp(ct))
            except json.JSONDecodeError:
                pass 

        if not new_lines: 
             if up.name.endswith('.jsonl'):
                for line in content.splitlines():
                    if line.strip():
                        try:
                            msg = json.loads(line)
                            if 'messages' in msg: new_lines.append(msg['messages'][0]['content'])
                        except: pass
             else:
                new_lines = [l.strip() for l in content.split('===SPLIT===') if l.strip()]
                if len(new_lines) < 2:
                     new_lines = [l.strip() for l in content.splitlines() if l.strip()]

        if new_lines:
            st.session_state.cached_data = {
                'lines': new_lines,
                'timestamps': new_timestamps,
                'sources': new_sources
            }
            
    except Exception as e:
        st.error(t('upload_error').format(e))

if st.session_state.cached_data:
    lines = st.session_state.cached_data['lines']
    timestamps = st.session_state.cached_data['timestamps']
    sources = st.session_state.cached_data['sources']

if not lines:
    show_onboarding_guide() # ğŸ‘ˆ è°ƒç”¨ç©ºçŠ¶æ€æŒ‡å¼•
    st.stop()

# --- æ•°æ®é¢„å¤„ç† ---
df = pd.DataFrame({"prompt": lines})
df["len"] = df["prompt"].str.len()

if timestamps and len(timestamps) == len(lines):
    df["time"] = timestamps
    df["hour"] = df["time"].dt.hour
    df["date"] = df["time"].dt.date
    df["weekday"] = df["time"].dt.weekday  # 0=Monday
    df["week_name"] = df["time"].dt.day_name()
    has_time = True
else:
    has_time = False

# --- æ ¸å¿ƒç®—æ³•ï¼šå¤æ‚åº¦è¯„åˆ† (Complexity Score 2.0) ---
def calculate_complexity(text):
    score = 0
    text_lower = text.lower()
    
    # 1. Base Score (Length)
    score += min(len(text) / 200, 1.0) * 30  # Reduced base weight
    
    # 2. Logical Depth
    logical_words = [
        'if', 'because', 'however', 'therefore', 'although', 'compare', 'difference',
        'å¦‚æœ', 'å› ä¸º', 'ä½†æ˜¯', 'æ‰€ä»¥', 'è™½ç„¶', 'æ¯”è¾ƒ', 'åŒºåˆ«', 'åŸç†', 'åˆ†æ', 'why', 'how',
        'strategy', 'plan', 'method', 'approach', 'framework', 'model', 'theory'
    ]
    logic_hits = sum(1 for w in logical_words if w in text_lower)
    score += min(logic_hits / 5, 1.0) * 25
    
    # 3. Structural Bonus (Markdown)
    structure_score = 0
    if '```' in text: structure_score += 15  # Code block
    if '\n-' in text or '\n*' in text: structure_score += 10  # List
    if '\n1.' in text: structure_score += 10 # Ordered list
    if '> ' in text: structure_score += 5    # Quote
    score += min(structure_score, 30)
    
    # 4. Cognitive Patterns (Role & CoT)
    cognitive_score = 0
    # Role Prompting
    if any(p in text_lower for p in ['act as', 'role', 'æ‰®æ¼”', 'ä½ æ˜¯ä¸€ä¸ª', 'you are a']):
        cognitive_score += 10
    # Chain of Thought
    if any(p in text_lower for p in ['step by step', 'chain of thought', 'reasoning', 'ä¸€æ­¥æ­¥', 'æ€ç»´é“¾']):
        cognitive_score += 15
    score += min(cognitive_score, 15)

    return min(int(score), 100)

df['complexity'] = df['prompt'].apply(calculate_complexity)

# --- æ ¸å¿ƒç®—æ³•ï¼šåŒè¯­åˆ†è¯ (Bilingual NLP + De-noising) ---
@st.cache_data
def process_tokens(text_list):
    all_text = " ".join(text_list)
    
    # 1. ä¸­æ–‡åˆ†è¯ (jieba) + å»åœç”¨è¯
    zh_pattern = re.compile(r'[\u4e00-\u9fa5]+')
    zh_words = [w for w in jieba.lcut(all_text) if len(w) > 1 and zh_pattern.match(w) and w not in chinese_stops]
    
    # 2. è‹±æ–‡åˆ†è¯ (ç®€å•æ­£åˆ™ + NLTK Stopwords)
    en_pattern = re.compile(r'[a-zA-Z]{2,}')
    en_words = en_pattern.findall(all_text.lower())
    en_words = [w for w in en_words if w not in english_stops and len(w) > 2] # Filter super short English words
    
    return zh_words + en_words

words = process_tokens(lines)
word_counts = Counter(words)

# --- Luxury Chart Helper ---
def luxury_chart(fig, title=None, show_median=False, df_col=None):
    fig.update_layout(
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)',
        font=dict(color='rgba(255,255,255,0.7)', family="Helvetica Neue"),
        title=dict(text=title, font=dict(color='#f8fafc', size=16)),
        xaxis=dict(showgrid=False, zeroline=False, color='rgba(255,255,255,0.4)'),
        yaxis=dict(showgrid=True, gridwidth=1, gridcolor='rgba(255,255,255,0.05)', zeroline=False, color='rgba(255,255,255,0.4)'),
        margin=dict(l=20, r=20, t=50, b=20),
        hovermode="x unified"
    )
    
    if show_median and df_col is not None:
        median = df_col.median()
        p90 = df_col.quantile(0.9)
        
        # Add Lines
        fig.add_vline(x=median, line_width=1, line_dash="dash", line_color="#D4AF37", opacity=0.8)
        fig.add_annotation(x=median, y=1, yref="paper", text=f"Med: {int(median)}", showarrow=False, font=dict(color="#D4AF37", size=10), yshift=10)
        
        fig.add_vline(x=p90, line_width=1, line_dash="dot", line_color="#6A5ACD", opacity=0.8)
        fig.add_annotation(x=p90, y=1, yref="paper", text=f"P90: {int(p90)}", showarrow=False, font=dict(color="#6A5ACD", size=10), yshift=10)

    return fig

# --- Dashboard æ¦‚è§ˆ ---
st.subheader(t('overview_header'))
c1, c2, c3, c4 = st.columns(4)
c1.metric(t('metric_total'), f"{len(df)}", delta=f"Avg Complexity: {int(df['complexity'].mean())}")
c2.metric(t('metric_vocab'), f"{len(word_counts)}")
c3.metric(t('metric_avg_len'), f"{int(df['len'].mean())}")
top_word = word_counts.most_common(1)[0][0] if word_counts else "N/A"
c4.metric(t('metric_top_word'), top_word)

st.divider()

# --- Tab å¸ƒå±€ ---
tab_insight, tab_habit, tab_data, tab_manage = st.tabs([t('tab_insight'), t('tab_habit'), t('tab_data'), t('tab_manage')])

# === Tab 1: æ€ç»´æ´å¯Ÿ ===
with tab_insight:
    col_radar, col_cloud = st.columns([1, 1.5])
    
    with col_radar:
        st.subheader(t('radar_header'))
        
        category_defs = {
            "coding": {
                "keywords": [
                    "ä»£ç ", "code", "å‡½æ•°", "æŠ¥é”™", "bug", "python", "js", "react", "sql", "api", "å†™ä¸€ä¸ª", "å®ç°", "function", "class", "error", "æ¥å£",
                    "java", "c++", "golang", "rust", "node", "css", "html", "docker", "k8s", "aws", "git", "github", "merge", "branch", "commit",
                    "database", "db", "mongo", "redis", "query", "request", "response", "json", "xml", "yaml", "config", "deploy", "build", "run",
                    "script", "algorithm", "loop", "variable", "import", "package", "install", "pip", "npm", "yarn", "compile", "debug", "stack",
                    "overflow", "exception", "try", "catch", "async", "await", "promise", "thread", "process", "linux", "shell", "bash", "terminal",
                    "fix", "issue", "crash", "stacktrace", "ci/cd", "pipeline", "jenkins", "azure", "gcp", "server", "client", "frontend", "backend",
                    "fullstack", "devops", "sre", "unit test", "integration test", "e2e", "selenium", "cypress", "playwright", "jest", "mocha",
                    "typescript", "ts", "vue", "angular", "svelte", "nextjs", "nuxtjs", "flask", "django", "fastapi", "spring", "hibernate",
                    "refactor", "optimize", "performance", "memory", "cpu", "leak", "profiling", "benchmark", "logging", "monitoring", "alerting",
                    "rest", "graphql", "grpc", "websocket", "socket", "tcp", "udp", "http", "https", "ssl", "tls", "certificate", "key", "token",
                    "auth", "jwt", "oauth", "sso", "ldap", "encryption", "hashing", "salt", "uuid", "guid", "regex", "regular expression"
                ],
                "label_en": "Coding",
                "label_zh": "ğŸ’» ç¼–ç¨‹å¼€å‘"
            },
            "writing": {
                "keywords": [
                    "æ–‡æ¡ˆ", "æ–‡ç« ", "å‘¨æŠ¥", "æ€»ç»“", "æ‰©å†™", "æ¶¦è‰²", "å¤§çº²", "æ ‡é¢˜", "ç¿»è¯‘", "é‚®ä»¶", "write", "email", "article", "summary", "translate", "outline", "title",
                    "essay", "blog", "post", "copy", "copywriting", "intro", "introduction", "conclusion", "paragraph", "sentence", "grammar", "spelling",
                    "tone", "style", "formal", "casual", "academic", "professional", "rewrite", "revise", "edit", "proofread", "check", "draft",
                    "report", "memo", "letter", "proposal", "statement", "bio", "description", "caption", "slogan", "tagline", "keyword", "seo",
                    "story", "narrative", "plot", "character", "dialogue", "script", "screenplay", "poem", "lyrics", "rhyme", "verse"
                ],
                "label_en": "Writing",
                "label_zh": "ğŸ“ å†…å®¹åˆ›ä½œ"
            },
            "logic": {
                "keywords": [
                    "åˆ†æ", "åŸå› ", "åŒºåˆ«", "æ¯”è¾ƒ", "è¯„ä»·", "ä¼˜ç¼ºç‚¹", "å»ºè®®", "æ–¹æ¡ˆ", "æ€ç»´å¯¼å›¾", "analyze", "reason", "compare", "difference", "pros", "cons", "plan",
                    "strategy", "tactic", "method", "approach", "framework", "model", "theory", "hypothesis", "assumption", "premise", "conclusion",
                    "argument", "debate", "critique", "review", "evaluate", "assess", "audit", "investigate", "research", "study", "survey", "data",
                    "evidence", "proof", "logic", "logical", "fallacy", "bias", "cognitive", "psychology", "philosophy", "ethics", "moral", "value",
                    "principle", "rule", "law", "regulation", "policy", "guideline", "standard", "criteria", "metric", "kpi", "okr", "goal", "objective"
                ],
                "label_en": "Logic",
                "label_zh": "ğŸ§  é€»è¾‘åˆ†æ"
            },
            "learning": {
                "keywords": [
                    "è§£é‡Š", "ä»‹ç»", "æ˜¯ä»€ä¹ˆ", "å«ä¹‰", "åŸç†", "æ•™ç¨‹", "å­¦ä¹ ", "å¦‚ä½•", "explain", "what", "how", "meaning", "tutorial", "principle", "learn",
                    "teach", "guide", "lesson", "course", "class", "lecture", "study", "exam", "test", "quiz", "question", "answer", "solution",
                    "definition", "define", "concept", "term", "vocabulary", "grammar", "history", "geography", "science", "math", "physics", "chemistry",
                    "biology", "art", "music", "literature", "culture", "language", "skill", "technique", "tip", "trick", "hack", "advice", "suggestion",
                    "recommendation", "resource", "book", "paper", "article", "video", "podcast", "website", "tool", "software", "app",
                    "roadmap", "path", "curriculum", "syllabus", "beginner", "intermediate", "advanced", "expert", "master", "pro", "101", "basics",
                    "fundamentals", "overview", "introduction", "background", "context", "history", "origin", "evolution", "development", "trend",
                    "future", "prediction", "forecast", "insight", "knowledge", "wisdom", "experience", "expertise", "mastery", "proficiency",
                    "competence", "capability", "ability", "talent", "gift", "aptitude", "potential", "growth", "development", "progress"
                ],
                "label_en": "Learning",
                "label_zh": "ğŸ“ çŸ¥è¯†å­¦ä¹ "
            },
            "creative": {
                "keywords": [
                    "åˆ›æ„", "ç‚¹å­", "æ•…äº‹", "è®¾æƒ³", "å¦‚æœ", "ç”Ÿæˆ", "è®¾è®¡", "idea", "story", "design", "imagine", "generate", "create",
                    "brainstorm", "concept", "inspiration", "vision", "dream", "fantasy", "fiction", "novel", "game", "play", "fun", "joke", "humor",
                    "comedy", "satire", "parody", "meme", "logo", "icon", "image", "picture", "photo", "video", "audio", "music", "song", "sound",
                    "color", "palette", "font", "typography", "layout", "ui", "ux", "wireframe", "prototype", "mockup", "sketch", "drawing", "painting",
                    "character", "role", "persona", "profile", "background", "backstory", "plot", "setting", "scene", "dialogue", "script", "screenplay",
                    "poem", "haiku", "limerick", "sonnet", "lyrics", "verse", "rhyme", "rhythm", "melody", "harmony", "chord", "scale", "key",
                    "style", "genre", "mood", "atmosphere", "vibe", "tone", "voice", "narrator", "perspective", "viewpoint", "theme", "motif",
                    "symbol", "metaphor", "simile", "analogy", "allegory", "fable", "myth", "legend", "folklore", "fairy tale", "fantasy", "sci-fi"
                ],
                "label_en": "Creative",
                "label_zh": "ğŸ¨ åˆ›æ„è„‘æš´"
            }
        }
        
        cat_scores = {k: 0 for k in category_defs.keys()}
        for w in words:
            for cat_key, cat_data in category_defs.items():
                if w in cat_data['keywords']:
                    cat_scores[cat_key] += 1
        
        vals = list(cat_scores.values())
        max_val = max(vals) if vals else 1
        normalized_vals = [v/max_val for v in vals]
        labels = [category_defs[k][f'label_{st.session_state.lang}'] for k in category_defs.keys()]
        
        fig_radar = px.line_polar(r=normalized_vals, theta=labels, line_close=True, template="plotly_dark")
        fig_radar.update_traces(fill='toself', line_color='#D4AF37') # Champagne Gold
        fig_radar.update_layout(
            polar=dict(
                radialaxis=dict(visible=False),
                bgcolor='rgba(0,0,0,0)'
            ),
            margin=dict(t=20, b=20, l=30, r=30),
            height=350,
            paper_bgcolor='rgba(0,0,0,0)',
            plot_bgcolor='rgba(0,0,0,0)'
        )
        st.plotly_chart(fig_radar, use_container_width=True)

    with col_cloud:
        st.subheader(t('cloud_header'))
        if words:
            # WordCloud with Luxury Colors
            def luxury_color_func(word, font_size, position, orientation, random_state=None, **kwargs):
                return "hsl(46, 65%, 60%)" if random_state.randint(0, 1) else "hsl(245, 40%, 70%)"

            wc = WordCloud(font_path=font_path, width=800, height=500, 
                          background_color=None, mode="RGBA", # Transparent
                          max_words=80, collocations=False,
                          color_func=luxury_color_func).generate(" ".join(words))
            st.image(wc.to_array(), use_column_width=True)
        else:
            st.warning(t('cloud_warning'))

    st.subheader(t('dist_header'))
    c_len, c_comp = st.columns(2)
    
    with c_len:
        fig_len = px.histogram(
             df, x="len", nbins=30,
             color_discrete_sequence=['#D4AF37'], # Champagne Gold
             template="plotly_dark"
        )
        fig_len.update_traces(
            marker=dict(line=dict(width=1, color='rgba(255,255,255,0.5)'), pattern=dict(shape="/")), # Glassy Edge + Texture
            opacity=0.8
        )
        luxury_chart(fig_len, t('dist_len_title'), show_median=True, df_col=df['len'])
        st.plotly_chart(fig_len, use_container_width=True)

    with c_comp:
        fig_comp = px.histogram(
            df, x="complexity", nbins=20, 
            color_discrete_sequence=['#6A5ACD'], # Royal Purple
            template="plotly_dark"
        )
        fig_comp.update_traces(
            marker=dict(line=dict(width=1, color='rgba(255,255,255,0.5)'), pattern=dict(shape="+")), # Glassy Edge + Texture
            opacity=0.8
        )
        luxury_chart(fig_comp, t('dist_comp_title'), show_median=True, df_col=df['complexity'])
        st.plotly_chart(fig_comp, use_container_width=True)

    # æ¢å¤é«˜é¢‘è¯ç»„ (Bigrams) æ¿å—
    st.divider()
    st.subheader(t('phrases_header'))
    
    bigrams = []
    # Advanced Bigram Filter
    # Stopwords for Bigrams (Less aggressive)
    bigram_stops = {
        "the", "a", "an", "in", "on", "at", "for", "to", "of", "is", "are", "was", "were", 
        "be", "been", "being", "have", "has", "had", "do", "does", "did", "it", "that", 
        "this", "these", "those", "i", "you", "he", "she", "we", "they", "my", "your", 
        "his", "her", "our", "their", "and", "but", "or", "so", "not", "no", "yes", "please", 
        "me", "thanks", "thank", "want", "need", "like", "just", "get", "go", "know", "think", 
        "see", "say", "tell", "ask", "try", "look", "take", "give", "find", "use", "way", "new",
        "good", "great", "well", "much", "many", "lot", "little", "big", "small",
        "çš„", "äº†", "æ˜¯", "æˆ‘", "ä½ ", "ä»–", "åœ¨", "å’Œ", "æœ‰", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»", "èƒ½", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "æ€ä¹ˆ", "ä»€ä¹ˆ", "è¿™", "é‚£", "è¿™ä¸ª", "é‚£ä¸ª", "è¯·", "å¸®æˆ‘", "ç»™æˆ‘", "å¯ä»¥", "å—",
        "ä¸ª", "åª", "æ¬¡", "æŠŠ", "è¢«", "è®©", "ç»™", "ä½†", "å› ä¸º", "æ‰€ä»¥", "å¦‚æœ", "è™½ç„¶", "ä½†æ˜¯", "æˆ–è€…", "è¿˜æ˜¯", "ä»¥åŠ", "é™¤äº†", "ä¸ºäº†", "å…³äº", "å¯¹äº", "é€šè¿‡", "æ ¹æ®", "æŒ‰ç…§", "ä½œä¸º", "éšç€"
    }

    for line in lines:
        # Tokenize line again but use the robust filter
        line_tokens = []
        # Chinese
        zh_pattern = re.compile(r'[\u4e00-\u9fa5]+')
        for w in jieba.lcut(line):
            if len(w) > 1 and zh_pattern.match(w) and w not in bigram_stops:
                line_tokens.append(w)
        # English
        en_pattern = re.compile(r'[a-zA-Z]{2,}')
        for w in en_pattern.findall(line.lower()):
            if w not in bigram_stops and len(w) > 2:
                line_tokens.append(w)
        
        if len(line_tokens) >= 2:
            for i in range(len(line_tokens)-1):
                # Filter meaningless bigrams
                w1, w2 = line_tokens[i], line_tokens[i+1]
                if w1 in bigram_stops or w2 in bigram_stops: continue
                bigrams.append(f"{w1} {w2}")

    top_bigrams = Counter(bigrams).most_common(12)

    # HTML/CSS Visuals for Top Phrases
    st.markdown("""
    <style>
    .phrase-tag {
        display: inline-block;
        padding: 6px 12px;
        margin: 4px;
        border-radius: 20px;
        color: #fff;
        font-size: 14px;
        font-weight: 500;
        backdrop-filter: blur(4px);
        border: 1px solid rgba(255,255,255,0.1);
        transition: transform 0.2s;
    }
    .phrase-tag:hover {
        transform: scale(1.05);
        border-color: #D4AF37;
    }
    </style>
    """, unsafe_allow_html=True)

    if top_bigrams:
        max_count = top_bigrams[0][1]
        html = "<div style='text-align: center; padding: 20px;'>"
        for phrase, count in top_bigrams:
            # Opacity based on frequency
            opacity = 0.3 + 0.7 * (count / max_count)
            # Gold color with varying opacity
            bg_color = f"rgba(212, 175, 55, {opacity})" 
            html += f"<span class='phrase-tag' style='background: {bg_color};' title='Count: {count}'>{phrase}</span>"
        html += "</div>"
        st.markdown(html, unsafe_allow_html=True)
    else:
        st.info("ğŸ’¡ Not enough data to generate top phrases yet. Try adding more diverse prompts!")

    # === Tab 2: ä¹ æƒ¯è¿½è¸ª ===
    with tab_habit:
        if has_time:
            st.subheader(t('habit_heatmap_header'))
            
            daily_counts = df['date'].value_counts().reset_index()
            daily_counts.columns = ['date', 'count']
            daily_counts['date'] = pd.to_datetime(daily_counts['date'])
            
            c1, c2 = st.columns([2, 1])
            
            with c1:
                st.caption(t('trend_caption'))
                fig_trend = px.bar(daily_counts.sort_values('date'), x='date', y='count', 
                                  color='count', color_continuous_scale='Blues', template="plotly_dark")
                luxury_chart(fig_trend, title=t('trend_caption'))
                st.plotly_chart(fig_trend, use_container_width=True)
                
            with c2:
                st.caption(t('hour_caption'))
                hour_counts = df['hour'].value_counts().sort_index().reset_index()
                hour_counts.columns = ['hour', 'count']
                
                tab_bar, tab_line = st.tabs([t('tab_bar'), t('tab_line')])
                
                with tab_bar:
                    fig_bar = px.bar(
                        hour_counts, x='hour', y='count',
                        template="plotly_dark"
                    )
                    fig_bar.update_traces(marker_color='#D4AF37')
                    luxury_chart(fig_bar, title=t('hour_caption'))
                    fig_bar.update_layout(xaxis=dict(tickmode='linear', dtick=2))
                    st.plotly_chart(fig_bar, use_container_width=True)
                    
                with tab_line:
                    fig_hour = px.line(hour_counts, x='hour', y='count', markers=True, template="plotly_dark")
                    fig_hour.update_traces(line_color='#6A5ACD')
                    luxury_chart(fig_hour, title=t('hour_caption'))
                    st.plotly_chart(fig_hour, use_container_width=True)
                
        else:
            st.warning(t('habit_warning'))

    # === Tab 3: åŸå§‹æ•°æ® ===
    with tab_data:
        st.subheader(t('search_header'))
        
        col_search, col_filter = st.columns([3, 1])
        with col_search:
            q = st.text_input(t('search_placeholder'), placeholder="Python, Writing...")
        with col_filter:
            min_score = st.slider(t('min_score_label'), 0, 100, 0)
        
        filtered_df = df.copy()
        if q:
            filtered_df = filtered_df[filtered_df['prompt'].str.contains(q, case=False)]
        filtered_df = filtered_df[filtered_df['complexity'] >= min_score]
        
        if has_time:
            filtered_df['time_str'] = filtered_df['time'].dt.strftime('%Y-%m-%d %H:%M')
            show_cols = ['time_str', 'prompt', 'complexity', 'len']
        else:
            show_cols = ['prompt', 'complexity', 'len']
            
        st.dataframe(
            filtered_df[show_cols].sort_values('complexity', ascending=False),
            column_config={
                "prompt": st.column_config.TextColumn(t('col_content'), width="large"),
                "complexity": st.column_config.ProgressColumn(t('col_score'), format="%d", min_value=0, max_value=100),
                "len": st.column_config.NumberColumn(t('col_len')),
                "time_str": st.column_config.TextColumn(t('col_time'))
            },
            use_container_width=True,
            height=600,
            hide_index=True
        )

    # === Tab 4: æ•°æ®ç®¡ç† ===
    with tab_manage:
        st.subheader(t('manage_header'))
        
        if 'delete_indices' not in st.session_state:
            st.session_state.delete_indices = []

        manage_df = df.copy()
        if has_time:
            manage_df['time_str'] = manage_df['time'].dt.strftime('%Y-%m-%d %H:%M')
        else:
            manage_df['time_str'] = "N/A"
            
        manage_df['delete'] = False
        
        edited_df = st.data_editor(
            manage_df[['delete', 'prompt', 'time_str', 'len']],
            column_config={
                "delete": st.column_config.CheckboxColumn("Select", width="small"),
                "prompt": st.column_config.TextColumn(t('col_content'), width="large"),
                "time_str": st.column_config.TextColumn(t('col_time'), width="medium"),
                "len": st.column_config.NumberColumn(t('col_len'), width="small")
            },
            hide_index=True,
            use_container_width=True,
            height=500
        )
        
        to_delete = edited_df[edited_df['delete'] == True]
        
        if not to_delete.empty:
            if st.button(f"{t('delete_btn')} ({len(to_delete)})", type="primary"):
                # Perform deletion
                # We need to find the original indices. Since df matches lines/timestamps lists by index
                delete_indices = set(to_delete.index)
                
                new_lines = []
                new_timestamps = []
                new_sources = []
                
                for i in range(len(lines)):
                    if i not in delete_indices:
                        new_lines.append(lines[i])
                        if timestamps: new_timestamps.append(timestamps[i])
                        if sources: new_sources.append(sources[i])
                
                # Update Cache
                st.session_state.cached_data['lines'] = new_lines
                st.session_state.cached_data['timestamps'] = new_timestamps
                st.session_state.cached_data['sources'] = new_sources
                
                st.success("Deleted successfully! Reloading...")
                st.rerun()
